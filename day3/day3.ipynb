{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Python Bootcamp Day 3\n",
    "\n",
    "Prepared and presented by John Russell and Ian Hunt-Isaak\n",
    "\n",
    "This notebook is available on [Github]()\n",
    "\n",
    "### Outline for the Day\n",
    "- Making beautiful plots for presentations\n",
    "- Crash course in covariance\n",
    "- Application: financial time series data\n",
    "- Time for questions, help, and other applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Beautiful Plots\n",
    "\n",
    "It is one thing to make a plot for yourself, e.g. to make sure a function does what you want to do some preliminary data visualization. But when the time comes to turn your plot in as part of an assignment or to present in slides, you need to make the plots clear and readable for your audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "#import matplotlib as mpl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate some data to plot - use lorenz equations from yesterday\n",
    "\n",
    "def lorenz(t, r, rho, sigma, beta):\n",
    "    x,y,z = r\n",
    "    dxdt = sigma*(y-x)\n",
    "    dydt = x*(rho-z) - y\n",
    "    dzdt = x*y-beta*z\n",
    "    return np.array([dxdt, dydt, dzdt])\n",
    "\n",
    "init_vals = np.array([1,1,1])\n",
    "lorenz_sol = solve_ivp(lorenz, (0,100), init_vals, t_eval = np.linspace(0,100,50000), args=(28, 10, 8/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plt.plot(lorenz_sol.y[0], lorenz_sol.y[1])\n",
    "plt.savefig(\"bad_plot1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Look at this plot in google slides](https://docs.google.com/presentation/d/1m_e95QT_hWmRM7InbBNS_zTQLK61z9B0MyIgTXlMY-k/edit#slide=id.g7ddea531b5_0_45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whats wrong here?\n",
    "- Background - bad for notebook but maybe good for white slides\n",
    "- Line - too thick - cannot see all the features\n",
    "- Size - relatively small\n",
    "- Labels - title is uninformative, no labels (or legend though not relevant here)\n",
    "- Font - too small for people to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are some matplot lib configurations that I like to use. \n",
    "import matplotlib as mpl\n",
    "mpl.rc(\"font\", family = \"serif\") #Serif font in matplotlib\n",
    "mpl.rc(\"figure\",figsize=(9,6)) #Increase default figure size\n",
    "%config InlineBackend.figure_format = 'retina' #Render the plots more nicely\n",
    "\n",
    "#the below make plots look better if youre using the dark theme for jupyter\n",
    "mpl.style.use('dark_background') #Use a dark background for matplotlib figures \n",
    "plt.rcParams.update({\"figure.facecolor\": \"111111\", #show figures here with a matching background\n",
    "                     \"savefig.facecolor\": \"212121\"}) #save figures with the color of google slides \"Simple Dark\" Theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#improved version\n",
    "\n",
    "plt.figure(figsize=(9,6))#arguments are width and height in inches\n",
    "plt.plot(lorenz_sol.y[0], lorenz_sol.y[1], linewidth = .75) #make line narrow to show all features. For simple plots they should be thicker.\n",
    "plt.title('Two Dimensions of the Lorenz Attractor',fontsize=20)\n",
    "plt.xlabel('X',fontsize=14)\n",
    "plt.ylabel('Y', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('good_plot1.png',dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance and introduction to Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([4.5,3])\n",
    "cov = np.array([[9, 5],\n",
    "                [5, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = np.random.multivariate_normal(mean, cov, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "plt.scatter(pts[:,0],pts[:,1], c='xkcd:peacock blue')\n",
    "plt.title(\"Some synthetic data - Positive Covariance\", fontsize=20)\n",
    "plt.xlim([-5,13])\n",
    "plt.xlim([-6,12])\n",
    "plt.xlabel(\"X\",fontsize=14)\n",
    "plt.ylabel(\"Y\",fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cov = np.array([[9, -5],\n",
    "                    [-5, 8]])\n",
    "\n",
    "neg_cov_pts = np.random.multivariate_normal(mean, neg_cov, 300)\n",
    "\n",
    "no_cov = np.array([[9, 0],\n",
    "                   [0, 8]])\n",
    "\n",
    "no_cov_pts = np.random.multivariate_normal(mean, no_cov, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(18,9))\n",
    "ax[0].scatter(no_cov_pts[:,0],no_cov_pts[:,1],color='xkcd:pinkish purple')\n",
    "ax[0].set_title('Data with No covariance',fontsize=18)\n",
    "ax[1].scatter(neg_cov_pts[:,0],neg_cov_pts[:,1], color='xkcd:dark yellow')\n",
    "ax[1].set_title('Data with Negative covariance',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we know the covariance stucture of our data?\n",
    "\n",
    "What we want to compute is called the covariance matrix. If that matrix is called $C$ and we want to know how a variable $x_i$ covaries with $x_j$ then we just need to look at $C_{i,j}$. This is all super easy in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_cov = np.cov(pts.T) #transpose it because what we want is how X covarys with Y not how each point covaries with each other point\n",
    "                              # I acutally got this wrong at first but its easy to check the shape of the covariance matrix to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(empirical_cov)\n",
    "print('Covariance of X and Y:', empirical_cov[0,1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets just work with the positive covariance data for now\n",
    "\n",
    "If we call our current variables $x$ and $y$ then there is a sense that they aren't the \"best\" axes to view our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(pts[:,0],pts[:,1], c='xkcd:peacock blue')\n",
    "plt.title(\"Some synthetic data - Positive Covariance\", fontsize=20)\n",
    "plt.xlim([-5,13])\n",
    "plt.xlim([-6,12])\n",
    "plt.xlabel(\"X\",fontsize=14)\n",
    "plt.ylabel(\"Y\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) tells us that the \"best\" axes to look at are the eigenvectors of the covariance matrix.\n",
    "\n",
    "Dont worry if you haven't seen eigen things before. In brief if $A$ is a matrix, $x$ is a vector, and\n",
    "\n",
    "$Ax = \\lambda x,$\n",
    "\n",
    "where $\\lambda$ is a scalar, then we say $x$ is an eigenvector of $A$ with eigenvalue $\\lambda$.\n",
    "\n",
    "How do we represent our data according to the \"best\" axes or pricipal components? Relatively simple matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eigh(empirical_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = pts@eigenvectors\n",
    "\n",
    "# one trick here = it helps to reverse the order of eigenvectors for PCA its best to sort by descending eigenvalue\n",
    "# numpy returns things in order of ascending eigenvalue\n",
    "transformed = transformed[:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(pts[:,0],pts[:,1], c='xkcd:peacock blue')\n",
    "plt.title(\"Some synthetic data - Positive Covariance\", fontsize=20)\n",
    "plt.xlim([-5,13])\n",
    "plt.xlim([-6,12])\n",
    "plt.xlabel(\"X\",fontsize=14)\n",
    "plt.ylabel(\"Y\",fontsize=14)\n",
    "plt.arrow(mean[0],mean[1],-np.sqrt(vals[0])*vecs[0,0],-np.sqrt(vals[0])*vecs[1,0],\n",
    "          head_width=0.5,fc='xkcd:mango', ec='xkcd:mango')\n",
    "plt.arrow(mean[0],mean[1],np.sqrt(vals[1])*vecs[0,1], np.sqrt(vals[1])*vecs[1,1],\n",
    "          head_width=0.5,fc='xkcd:mango', ec='xkcd:mango')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(transformed[:,0], transformed[:,1],c='xkcd:peacock blue')\n",
    "plt.title(\"Same syntetic data plotted by Principal Components\", fontsize=20)\n",
    "plt.ylim([-8,8])\n",
    "plt.xlim([-14,4])\n",
    "plt.xlabel('PC1',fontsize=14)\n",
    "plt.ylabel('PC2', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Math behind Principal Component Analysis - Variance and Covariance\n",
    "\n",
    "*This is beyond the scope of this bootcamp but provides some math equations to explain Covariance and PCA*\n",
    "\n",
    "Many of you have probably taken a statistics class and had to compute the variance or standard deviation of some data. As a refresher, if we have $N$ observations of a random variable $x$ then the variance is\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{N}\\sum_{n=1}^{N} (x_n-\\mu)^2$$\n",
    "\n",
    "Where $\\mu$ is the mean or average of the $x$ values in the dataset.\n",
    "\n",
    "The covariance generalizes this concept to the case where one is interested in multiple random variables that may be correlated. If $x$ and $y$ are random variables which we have observed $N$ times then we can say\n",
    "\n",
    "$$\\text{Cov}(x,y) = \\frac{1}{N} \\sum_{n=1}^{N} (x_n-\\mu_x)(y_n -\\mu_y).$$\n",
    "\n",
    "Note the follwing important relations:\n",
    "\n",
    "$$\\text{Cov}(x,x) = \\sigma^2_x$$\n",
    "$$ \\text{Cov}(x,y) = \\text{Cov}(y,x).$$\n",
    "\n",
    "In general if there are $M$ random variables lets call them $x^{(i)}$ that may be correlated it is quite convenient to consider a *Covariance matrix* this a matrix $C$ such that \n",
    "\n",
    "$$C_{i,j} = \\text{Cov}(x^{(i)},x^{(j)}).$$\n",
    "\n",
    "Note that the two identies discussed above imply that \n",
    "1. The diagonal elements of the covariance matrix are the variances of each variable and\n",
    "1. The covariance matrix is symmetric and\n",
    "1. For the mathematicians it is also positive definite since variances are always positive. \n",
    "\n",
    "\n",
    "The covariance of different variables in a dataset is one of the most important things one can learn about their data and there are lots of ways of using the information in the covariance matrix to analyze and understand the data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
